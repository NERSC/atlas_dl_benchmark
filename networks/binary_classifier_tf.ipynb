{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*** License Agreement ***                                                                                                                                                                                                                                                                                  \n",
    "#                                                                                                                                                                                                                                                                                                           \n",
    "#High Energy Physics Deep Learning Convolutional Neural Network Benchmark (HEPCNNB) Copyright (c) 2017, The Regents of the University of California,                                                                                                                                                        \n",
    "#through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy). All rights reserved.                                                                                                                                                           \n",
    "#                                                                                                                                                                                                                                                                                                           \n",
    "#Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:                                                                                                                                                             \n",
    "#(1) Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.                                                                                                                                                                           \n",
    "#(2) Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer                                                                                                                                                                         \n",
    "#in the documentation and/or other materials provided with the distribution.                                                                                                                                                                                                                                \n",
    "#(3) Neither the name of the University of California, Lawrence Berkeley National Laboratory, U.S. Dept. of Energy nor the names                                                                                                                                                                            \n",
    "#of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.                                                                                                                                                                       \n",
    "#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,                                                                                                                                                                              \n",
    "#BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE                                                                                                                                                                   \n",
    "#COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT                                                                                                                                                           \n",
    "#LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF                                                                                                                                                      \n",
    "#LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,                                                                                                                                                          \n",
    "#EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "#You are under no obligation whatsoever to provide any bug fixes, patches, or upgrades to the features,                                                                                                                                                                                                     \n",
    "#functionality or performance of the source code (\"Enhancements\") to anyone; however,                                                                                                                                                                                                                       \n",
    "#if you choose to make your Enhancements available either publicly, or directly to Lawrence Berkeley National Laboratory,                                                                                                                                                                                   \n",
    "#without imposing a separate written license agreement for such Enhancements, then you hereby grant the following license: a non-exclusive,                                                                                                                                                                 \n",
    "#royalty-free perpetual license to install, use, modify, prepare derivative works, incorporate into other computer software,                                                                                                                                                                                \n",
    "#distribute, and sublicense such enhancements or derivative works thereof, in binary and source code form.                                                                                                                                                                                                  \n",
    "#---------------------------------------------------------------      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os stuff\n",
    "import os\n",
    "import h5py as h5\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "from numpy.random import RandomState as rng\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as tfk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \n",
    "    def reset(self):\n",
    "        self._epochs_completed = 0\n",
    "        self._file_index = 0\n",
    "        self._data_index = 0\n",
    "    \n",
    "    \n",
    "    def load_next_file(self):\n",
    "        #only load a new file if there are more than one file in the list:\n",
    "        if self._num_files > 1 or not self._initialized:\n",
    "            try:\n",
    "                with h5.File(self._filelist[self._file_index],'r') as f:\n",
    "                    #determine total array size:\n",
    "                    numentries=f['data'].shape[0]\n",
    "                \n",
    "                    if self._split_file:\n",
    "                        blocksize = int(np.ceil(numentries/float(self._num_tasks)))\n",
    "                        start = self._taskid*blocksize\n",
    "                        end = (self._taskid+1)*blocksize\n",
    "                    else:\n",
    "                        start = 0\n",
    "                        end = numentries\n",
    "                \n",
    "                    #load the chunk which is needed\n",
    "                    self._images = f['data'][start:end]\n",
    "                    self._labels = f['label'][start:end]\n",
    "                    self._normweights = f['normweight'][start:end]\n",
    "                    self._weights = f['weight'][start:end]\n",
    "                    self._psr = f['psr'][start:end]\n",
    "                    f.close()\n",
    "            except EnvironmentError:\n",
    "                raise EnvironmentError(\"Cannot open file \"+self._filelist[self._file_index])\n",
    "                \n",
    "            #sanity checks\n",
    "            assert self._images.shape[0] == self._labels.shape[0], ('images.shape: %s labels.shape: %s' % (self._images.shape, self_.labels.shape))\n",
    "            assert self._labels.shape[0] == self._normweights.shape[0], ('labels.shape: %s normweights.shape: %s' % (self._labels.shape, self._normweights.shape))\n",
    "            assert self._labels.shape[0] == self._psr.shape[0], ('labels.shape: %s psr.shape: %s' % (self._labels.shape, self._psr.shape))\n",
    "            self._initialized = True\n",
    "        \n",
    "            #set number of samples\n",
    "            self._num_examples = self._labels.shape[0]\n",
    "        \n",
    "            #reshape labels and weights\n",
    "            self._labels = np.expand_dims(self._labels, axis=1).astype(np.int32, copy=False)\n",
    "            self._normweights = np.expand_dims(self._normweights, axis=1)\n",
    "            self._weights = np.expand_dims(self._weights, axis=1)\n",
    "            self._psr = np.expand_dims(self._psr, axis=1)\n",
    "            \n",
    "            #transpose images if data format is NHWC\n",
    "            if self._data_format == \"NHWC\":\n",
    "                #transform for NCHW to NHWC\n",
    "                self._images = np.transpose(self._images, (0,2,3,1))\n",
    "            \n",
    "        #create permutation\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        #shuffle\n",
    "        self._images = self._images[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        self._normweights = self._normweights[perm]\n",
    "        self._weights = self._weights[perm]\n",
    "        self._psr = self._psr[perm]\n",
    "        \n",
    "    \n",
    "    def __init__(self, filelist,num_tasks=1,taskid=0,split_filelist=False,split_file=False,data_format=\"NCHW\"):\n",
    "        \"\"\"Construct DataSet\"\"\"\n",
    "        #multinode stuff\n",
    "        self._num_tasks = num_tasks\n",
    "        self._taskid = taskid\n",
    "        self._split_filelist = split_filelist\n",
    "        self._split_file = split_file\n",
    "        self._data_format = data_format\n",
    "        \n",
    "        #split filelist?\n",
    "        self._num_files = len(filelist)\n",
    "        start = 0\n",
    "        end = self._num_files\n",
    "        if self._split_filelist:\n",
    "            self._num_files = int(np.floor(len(filelist)/float(self._num_tasks)))\n",
    "            start = self._taskid * self._num_files\n",
    "            end = start + self._num_files\n",
    "        \n",
    "        assert self._num_files > 0, ('filelist is empty')\n",
    "        \n",
    "        self._filelist = filelist[start:end]\n",
    "        self._initialized = False\n",
    "        self.reset()\n",
    "        self.load_next_file()\n",
    "\n",
    "    @property\n",
    "    def num_files(self):\n",
    "        return self._num_files\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._data_index\n",
    "        self._data_index += batch_size\n",
    "        end=int(np.min([self._num_examples,self._data_index]))\n",
    "        \n",
    "        #take what is there\n",
    "        images = self._images[start:end]\n",
    "        labels = self._labels[start:end]\n",
    "        normweights = self._normweights[start:end]\n",
    "        weights = self._weights[start:end]\n",
    "        psr = self._psr[start:end]\n",
    "        \n",
    "        if self._data_index > self._num_examples:\n",
    "            #remains:\n",
    "            remaining = self._data_index-self._num_examples\n",
    "            \n",
    "            #first, reset data_index and increase file index:\n",
    "            self._data_index=0\n",
    "            self._file_index+=1\n",
    "            \n",
    "            #check if we are at the end of the file list\n",
    "            if self._file_index >= self._num_files:\n",
    "                #epoch is finished\n",
    "                self._epochs_completed += 1\n",
    "                #reset file index and shuffle list\n",
    "                self._file_index=0\n",
    "                np.random.shuffle(self._filelist)\n",
    "            \n",
    "            #load the next file\n",
    "            self.load_next_file()\n",
    "            #assert batch_size <= self._num_examples\n",
    "            #call rerucsively\n",
    "            tmpimages,tmplabels,tmpnormweights,tmpweights,tmppsr = self.next_batch(remaining)\n",
    "            #join\n",
    "            images = np.concatenate([images,tmpimages],axis=0)    \n",
    "            labels = np.concatenate([labels,tmplabels],axis=0)\n",
    "            normweights = np.concatenate([normweights,tmpnormweights],axis=0)\n",
    "            weights = np.concatenate([weights,tmpweights],axis=0)\n",
    "            psr = np.concatenate([psr,tmppsr],axis=0)\n",
    "        \n",
    "        return images, labels, normweights, weights, psr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummySet(object):\n",
    "    \n",
    "    def reset(self):\n",
    "        self._random = rng(self._seed)\n",
    "        self._data_index = 0\n",
    "        self._epochs_completed = 0\n",
    "        \n",
    "    def __init__(self, input_shape, samples_per_epoch, task_index=1):\n",
    "        self._seed = task_index * 13\n",
    "        self._shape = input_shape\n",
    "        self._datasize = int(np.prod(self._shape))\n",
    "        self._samples_per_epoch = samples_per_epoch\n",
    "        self.reset()\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        data = np.reshape(self._random.rand(self._datasize*batch_size), [batch_size]+self._shape)\n",
    "        labels = np.expand_dims(self._random.random_integers(0, 1, batch_size),1)\n",
    "        normweights = np.expand_dims(self._random.rand(batch_size),1)\n",
    "        weights = normweights\n",
    "        psr = labels\n",
    "        \n",
    "        #increase data counter and check if epoch finished\n",
    "        self._data_index += batch_size\n",
    "        if self._data_index >= self._samples_per_epoch:\n",
    "            self._data_index = 0\n",
    "            self._epochs_completed += 1\n",
    "        \n",
    "        return data, labels, normweights, weights, psr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEP CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(args):\n",
    "    \n",
    "    #datatype\n",
    "    dtype=args[\"precision\"]\n",
    "    \n",
    "    #find out which device to use:\n",
    "    device='/cpu:0'\n",
    "    if args['arch']=='gpu':\n",
    "        device='/gpu:0'\n",
    "    \n",
    "    #define empty variables dict\n",
    "    variables={}\n",
    "    \n",
    "    #rotate input shape depending on data format\n",
    "    data_format=args['conv_params']['data_format']\n",
    "    input_shape = args['input_shape']\n",
    "    \n",
    "    #create placeholders\n",
    "    variables['images_'] = tf.placeholder(dtype, shape=[args['train_batch_size_per_node']]+input_shape)\n",
    "    variables['keep_prob_'] = tf.placeholder(dtype)\n",
    "    \n",
    "    #empty network:\n",
    "    network = []\n",
    "    \n",
    "    #input layer\n",
    "    network.append(tf.reshape(variables['images_'], [-1]+input_shape, name='input'))\n",
    "    \n",
    "    #get all the conv-args stuff:\n",
    "    activation=args['conv_params']['activation']\n",
    "    initializer=args['conv_params']['initializer']\n",
    "    ksize=args['conv_params']['filter_size']\n",
    "    num_filters=args['conv_params']['num_filters']\n",
    "    padding=args['conv_params']['padding']\n",
    "        \n",
    "    #conv layers:\n",
    "    prev_num_filters=args['input_shape'][0]\n",
    "    if data_format==\"NHWC\":\n",
    "        prev_num_filters=args['input_shape'][2]\n",
    "        \n",
    "    for layerid in range(1,args['num_layers']+1):\n",
    "        \n",
    "        #create weight-variable\n",
    "        #with tf.device(device):\n",
    "        variables['conv'+str(layerid)+'_w']=tf.Variable(initializer([ksize,ksize,prev_num_filters,num_filters],dtype=dtype),\n",
    "                                                        name='conv'+str(layerid)+'_w',dtype=dtype)\n",
    "        prev_num_filters=num_filters\n",
    "        \n",
    "        #conv unit\n",
    "        network.append(tf.nn.conv2d(network[-1],\n",
    "                                    filter=variables['conv'+str(layerid)+'_w'],\n",
    "                                    strides=[1, 1, 1, 1], \n",
    "                                    padding=padding,\n",
    "                                    data_format=data_format,\n",
    "                                    name='conv'+str(layerid)))\n",
    "        \n",
    "        #batchnorm if desired\n",
    "        outshape=network[-1].shape[1:]\n",
    "        if args['batch_norm']:\n",
    "            #add batchnorm\n",
    "            #with tf.device(device):\n",
    "            #mu\n",
    "            variables['bn'+str(layerid)+'_m']=tf.Variable(tf.zeros(outshape),\n",
    "                                                          name='bn'+str(layerid)+'_m',dtype=dtype)\n",
    "            #sigma\n",
    "            variables['bn'+str(layerid)+'_s']=tf.Variable(tf.ones(outshape),\n",
    "                                                          name='bn'+str(layerid)+'_s',dtype=dtype)\n",
    "            #gamma\n",
    "            variables['bn'+str(layerid)+'_g']=tf.Variable(tf.ones(outshape),\n",
    "                                                          name='bn'+str(layerid)+'_g',dtype=dtype)\n",
    "            #beta\n",
    "            variables['bn'+str(layerid)+'_b']=tf.Variable(tf.zeros(outshape),\n",
    "                                                          name='bn'+str(layerid)+'_b',dtype=dtype)\n",
    "            #add batch norm layer\n",
    "            network.append(tf.nn.batch_normalization(network[-1],\n",
    "                            mean=variables['bn'+str(layerid)+'_m'],\n",
    "                            variance=variables['bn'+str(layerid)+'_s'],\n",
    "                            offset=variables['bn'+str(layerid)+'_b'],\n",
    "                            scale=variables['bn'+str(layerid)+'_g'],\n",
    "                            variance_epsilon=1.e-4,\n",
    "                            name='bn'+str(layerid)))\n",
    "        #else:\n",
    "        #    #add simple bias\n",
    "        #    with tf.device(device): \n",
    "        #        #bias shape\n",
    "        #        bshape = (variables['conv'+str(layerid)+'_w'].shape[3])\n",
    "        #        variables['conv'+str(layerid)+'_b']=tf.Variable(tf.zeros(bshape),\n",
    "        #                                                     name='conv'+str(layerid)+'_b',dtype=dtype)\n",
    "        #        #add bias\n",
    "        #        network.append(tf.nn.bias_add(network[-1],variables['conv'+str(layerid)+'_b'],data_format=data_format))\n",
    "        \n",
    "        #add relu unit\n",
    "        #with tf.device(device):\n",
    "        network.append(activation(network[-1]))\n",
    "        \n",
    "        #add maxpool\n",
    "        #with tf.device(device):\n",
    "        kshape=[1,1,2,2]\n",
    "        sshape=[1,1,2,2]\n",
    "        if data_format==\"NHWC\":\n",
    "            kshape=[1,2,2,1]\n",
    "            sshape=[1,2,2,1]\n",
    "        network.append(tf.nn.max_pool(network[-1],\n",
    "                                      ksize=kshape,\n",
    "                                      strides=sshape,\n",
    "                                      padding=args['conv_params']['padding'],\n",
    "                                      data_format=data_format,\n",
    "                                      name='maxpool'+str(layerid)))\n",
    "        \n",
    "        #add dropout\n",
    "        #with tf.device(device):\n",
    "        network.append(tf.nn.dropout(network[-1],\n",
    "                                     keep_prob=variables['keep_prob_'],\n",
    "                                     name='drop'+str(layerid)))\n",
    "    \n",
    "    if args['scaling_improvements']:\n",
    "        #add another conv layer with average pooling to the mix\n",
    "        #with tf.device(device):\n",
    "        variables['conv'+str(layerid+1)+'_w']=tf.Variable(initializer([ksize,ksize,prev_num_filters,num_filters],dtype=dtype),\n",
    "                                                          name='conv'+str(layerid+1)+'_w',dtype=dtype)\n",
    "        prev_num_filters=num_filters\n",
    "        \n",
    "        #conv unit\n",
    "        network.append(tf.nn.conv2d(network[-1],\n",
    "                                    filter=variables['conv'+str(layerid+1)+'_w'],\n",
    "                                    strides=[1, 1, 1, 1], \n",
    "                                    padding=padding,\n",
    "                                    data_format=data_format,\n",
    "                                    name='conv'+str(layerid+1)))\n",
    "            \n",
    "        #add relu unit\n",
    "        #with tf.device(device):\n",
    "        network.append(activation(network[-1]))\n",
    "        \n",
    "        #add average-pool\n",
    "        #with tf.device(device):\n",
    "        #pool over everything\n",
    "        imsize = network[-1].shape[2]\n",
    "        kshape = [1,1,imsize,imsize]\n",
    "        sshape = [1,1,imsize,imsize]\n",
    "        if data_format == \"NHWC\":\n",
    "            kshape = [1,imsize,imsize,1]\n",
    "            sshape = [1,imsize,imsize,1]\n",
    "        network.append(tf.nn.avg_pool(network[-1],\n",
    "                                      ksize=kshape,\n",
    "                                      strides=sshape,\n",
    "                                      padding=args['conv_params']['padding'],\n",
    "                                      data_format=data_format,\n",
    "                                      name='avgpool1'))\n",
    "    \n",
    "    #reshape\n",
    "    outsize = np.prod(network[-1].shape[1:]).value\n",
    "    #with tf.device(device):\n",
    "    network.append(tf.reshape(network[-1],shape=[-1, outsize],name='flatten'))\n",
    "    \n",
    "    if not args['scaling_improvements']:\n",
    "        #now do the MLP\n",
    "        #fc1\n",
    "        #with tf.device(device):\n",
    "        variables['fc1_w']=tf.Variable(initializer([outsize, args['num_fc_units']],dtype=dtype),name='fc1_w',dtype=dtype)\n",
    "        variables['fc1_b']=tf.Variable(tf.zeros([args['num_fc_units']]),name='fc1_b',dtype=dtype)\n",
    "        network.append(tf.matmul(network[-1], variables['fc1_w']) + variables['fc1_b'])\n",
    "    \n",
    "        #add relu unit\n",
    "        #with tf.device(device):\n",
    "        network.append(activation(network[-1]))\n",
    "    \n",
    "        #add dropout\n",
    "        #with tf.device(device):\n",
    "        network.append(tf.nn.dropout(network[-1],\n",
    "                                     keep_prob=variables['keep_prob_'],\n",
    "                                     name='drop'+str(layerid)))\n",
    "        #fc2\n",
    "        #with tf.device(device):\n",
    "        variables['fc2_w']=tf.Variable(initializer([args['num_fc_units'],2],dtype=dtype),name='fc2_w',dtype=dtype)\n",
    "        variables['fc2_b']=tf.Variable(tf.zeros([2]),name='fc2_b',dtype=dtype)\n",
    "        network.append(tf.matmul(network[-1], variables['fc2_w']) + variables['fc2_b'])\n",
    "            \n",
    "    else:\n",
    "        #only one FC layer here\n",
    "        #with tf.device(device):\n",
    "        variables['fc1_w']=tf.Variable(initializer([outsize,2],dtype=dtype),name='fc1_w',dtype=dtype)\n",
    "        variables['fc1_b']=tf.Variable(tf.zeros([2]),name='fc1_b',dtype=dtype)\n",
    "        network.append(tf.matmul(network[-1], variables['fc1_w']) + variables['fc1_b'])\n",
    "    \n",
    "    \n",
    "    #add softmax\n",
    "    #with tf.device(device):\n",
    "    network.append(tf.nn.softmax(network[-1]))\n",
    "    \n",
    "    #return the network and variables\n",
    "    return variables,network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Functions from the Network Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build the functions\n",
    "def build_functions(args,variables,network):\n",
    "    \n",
    "    #additional variables\n",
    "    variables['labels_']=tf.placeholder(tf.int32,shape=[args['train_batch_size_per_node'],1])\n",
    "    variables['weights_']=tf.placeholder(args[\"precision\"],shape=[args['train_batch_size_per_node'],1])\n",
    "    \n",
    "    #loss function\n",
    "    prediction = network[-1]\n",
    "    tf.add_to_collection('prediction_op', prediction)\n",
    "    \n",
    "    #compute loss, important: use unscaled version!\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(variables['labels_'],\n",
    "                                                  network[-2],\n",
    "                                                  weights=variables['weights_'])\n",
    "    \n",
    "    #compute accuracy\n",
    "    accuracy = tf.metrics.accuracy(variables['labels_'],\n",
    "                                   tf.round(prediction[:,1]),\n",
    "                                   weights=variables['weights_'],\n",
    "                                   name='accuracy')\n",
    "    \n",
    "    #compute AUC\n",
    "    auc = tf.metrics.auc(variables['labels_'],\n",
    "                         prediction[:,1],\n",
    "                         weights=variables['weights_'],\n",
    "                         num_thresholds=5000,\n",
    "                         curve='ROC',\n",
    "                         name='AUC')\n",
    "    \n",
    "    #get loss\n",
    "    return variables, prediction, loss, accuracy, auc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "thorstendl",
   "language": "python",
   "name": "thorstendl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
